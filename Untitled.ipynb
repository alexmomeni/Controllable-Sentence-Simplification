{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from access.preprocessors import get_preprocessors\n",
    "from access.resources.prepare import prepare_models\n",
    "from access.simplifiers import get_fairseq_simplifier, get_preprocessed_simplifier\n",
    "from access.text import word_tokenize\n",
    "from access.utils.helpers import yield_lines, write_lines, get_temp_filepath, mute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) Facebook, Inc. and its affiliates.\n",
    "# All rights reserved.\n",
    "#\n",
    "# This source code is licensed under the license found in the\n",
    "# LICENSE file in the root directory of this source tree.\n",
    "#\n",
    "\n",
    "from functools import lru_cache\n",
    "\n",
    "import Levenshtein\n",
    "import numpy as np\n",
    "\n",
    "from access.resources.paths import FASTTEXT_EMBEDDINGS_PATH\n",
    "from access.resources.prepare import prepare_fasttext_embeddings\n",
    "from access.text import (to_words, remove_punctuation_tokens, remove_stopwords, spacy_process)\n",
    "from access.utils.helpers import yield_lines\n",
    "\n",
    "\n",
    "def get_word2rank(vocab_size=np.inf):\n",
    "    prepare_fasttext_embeddings()\n",
    "    # TODO: Decrease vocab size or load from smaller file\n",
    "    word2rank = {}\n",
    "    line_generator = yield_lines(FASTTEXT_EMBEDDINGS_PATH)\n",
    "    next(line_generator)  # Skip the first line (header)\n",
    "    for i, line in enumerate(line_generator):\n",
    "        if (i + 1) > vocab_size:\n",
    "            break\n",
    "        word = line.split(' ')[0]\n",
    "        word2rank[word] = i\n",
    "    return word2rank\n",
    "\n",
    "\n",
    "def get_rank(word):\n",
    "    return get_word2rank().get(word, len(get_word2rank()))\n",
    "\n",
    "\n",
    "def get_log_rank(word):\n",
    "    return np.log(1 + get_rank(word))\n",
    "\n",
    "\n",
    "def get_lexical_complexity_score(sentence):\n",
    "    words = to_words(remove_stopwords(remove_punctuation_tokens(sentence)))\n",
    "    words = [word for word in words if word in get_word2rank()]\n",
    "    if len(words) == 0:\n",
    "        return np.log(1 + len(get_word2rank()))  # TODO: This is completely arbitrary\n",
    "    return np.quantile([get_log_rank(word) for word in words], 0.75)\n",
    "\n",
    "\n",
    "def get_levenshtein_similarity(complex_sentence, simple_sentence):\n",
    "    return Levenshtein.ratio(complex_sentence, simple_sentence)\n",
    "\n",
    "\n",
    "def get_dependency_tree_depth(sentence):\n",
    "    def get_subtree_depth(node):\n",
    "        if len(list(node.children)) == 0:\n",
    "            return 0\n",
    "        return 1 + max([get_subtree_depth(child) for child in node.children])\n",
    "\n",
    "    tree_depths = [get_subtree_depth(spacy_sentence.root) for spacy_sentence in spacy_process(sentence).sents]\n",
    "    if len(tree_depths) == 0:\n",
    "        return 0\n",
    "    return max(tree_depths)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "... 100% - 1264 MB - 11.92 MB/s - 106s\n",
      "Extracting...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-cc679c2b3786>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"As he crossed toward the pharmacy at the corner he involuntarily turned his head because of a burst of light that had ricocheted from his temple, and saw, with that quick smile with which we greet a rainbow or a rose, a blindingly white parallelogram of sky being unloaded from the van—a dresser with mirrors across which, as across a cinema screen, passed a flawlessly clear reflection of boughs sliding and swaying not arboreally, but with a human vacillation, produced by the nature of those who were carrying this sky, these boughs, this gliding façade.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mget_lexical_complexity_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-1-613ec69f4aeb>\u001b[0m in \u001b[0;36mget_lexical_complexity_score\u001b[0;34m(sentence)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_lexical_complexity_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mremove_stopwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mremove_punctuation_tokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m     \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwords\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mget_word2rank\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_word2rank\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# TODO: This is completely arbitrary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-613ec69f4aeb>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_lexical_complexity_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mremove_stopwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mremove_punctuation_tokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m     \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwords\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mget_word2rank\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_word2rank\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# TODO: This is completely arbitrary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-613ec69f4aeb>\u001b[0m in \u001b[0;36mget_word2rank\u001b[0;34m(vocab_size)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mword\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0mword2rank\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mword2rank\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "sentence = \"As he crossed toward the pharmacy at the corner he involuntarily turned his head because of a burst of light that had ricocheted from his temple, and saw, with that quick smile with which we greet a rainbow or a rose, a blindingly white parallelogram of sky being unloaded from the van—a dresser with mirrors across which, as across a cinema screen, passed a flawlessly clear reflection of boughs sliding and swaying not arboreally, but with a human vacillation, produced by the nature of those who were carrying this sky, these boughs, this gliding façade.\"\n",
    "get_lexical_complexity_score(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import shutil\n",
    "import sys\n",
    "import tempfile\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_filepath = get_temp_filepath()\n",
    "write_lines([word_tokenize(line) for line in fileinput.input()], source_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"source.complex\",'r') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/var/folders/bc/fl55zrvs7l102488vgch97qm0000gn/T/tmpgy9otb4m\n",
      "Some trails are designated as nature trails , and are used by people learning about the natural world .\n"
     ]
    }
   ],
   "source": [
    "source_filepath = get_temp_filepath()\n",
    "f = open(\"source.complex\",'r') \n",
    "l = [word_tokenize(line) for line in f]\n",
    "\n",
    "print(source_filepath)\n",
    "\n",
    "filepath = Path(source_filepath)\n",
    "filepath.parent.mkdir(parents=True, exist_ok=True)\n",
    "with filepath.open('w') as f:\n",
    "    for line in l:\n",
    "        print(line)\n",
    "        f.write(line + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/var/folders/bc/fl55zrvs7l102488vgch97qm0000gn/T/tmp6m0731wy\n",
      "<function get_fairseq_simplifier.<locals>.fairseq_simplifier at 0x133e58488>\n"
     ]
    }
   ],
   "source": [
    "best_model_dir = prepare_models()\n",
    "# Load best model\n",
    "#recommended_preprocessors_kwargs = {'LengthRatioPreprocessor': {'target_ratio': 0.25},\n",
    " #                                   'LevenshteinPreprocessor': {'target_ratio': 0.25},\n",
    "  #                                  'WordRankRatioPreprocessor': {'target_ratio': 0.25},\n",
    "   #                                 'SentencePiecePreprocessor': {'vocab_size': 10000},\n",
    "    #                               }\n",
    "\n",
    "preprocessors = get_preprocessors(recommended_preprocessors_kwargs)\n",
    "simplifier = get_fairseq_simplifier(best_model_dir)\n",
    "simplifier = get_preprocessed_simplifier(simplifier, preprocessors=None)\n",
    "\n",
    "pred_filepath = get_temp_filepath()\n",
    "print(pred_filepath)\n",
    "print(simplifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessors = get_preprocessors(recommended_preprocessors_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_preprocessed_simplifier(simplifier, preprocessors):\n",
    "    composed_preprocessor = ComposedPreprocessor(preprocessors)\n",
    "\n",
    "    @memoize_simplifier\n",
    "    @wraps(simplifier)\n",
    "    def preprocessed_simplifier(complex_filepath, output_pred_filepath):\n",
    "        print(f'preprocessors={preprocessors}')\n",
    "        preprocessed_complex_filepath = tempfile.mkstemp()[1]\n",
    "        pri\n",
    "        composed_preprocessor.encode_file(complex_filepath, preprocessed_complex_filepath)\n",
    "        preprocessed_output_pred_filepath = tempfile.mkstemp()[1]\n",
    "        simplifier(preprocessed_complex_filepath, preprocessed_output_pred_filepath)\n",
    "        composed_preprocessor.decode_file(preprocessed_output_pred_filepath,\n",
    "                                          output_pred_filepath,\n",
    "                                          encoder_filepath=complex_filepath)\n",
    "\n",
    "    preprocessed_simplifier.__name__ = f'{preprocessed_simplifier.__name__}_{composed_preprocessor.get_suffix()}'\n",
    "    return preprocessed_simplifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessors=[LengthRatioPreprocessor(target_ratio=0.25), LevenshteinPreprocessor(bucket_size=0.05, noise_std=0, target_ratio=0.25), WordRankRatioPreprocessor(target_ratio=0.25), SentencePiecePreprocessor(input_filepaths=None, vocab_size=10000)]\n",
      "simplifier_type=\"fairseq_simplifier\"  \n",
      "exp_dir=\"/Users/alex/personal_projects/access/resources/models/best_model\"  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--no-progress-bar] [--log-interval N]\n",
      "                             [--log-format {json,none,simple,tqdm}]\n",
      "                             [--tensorboard-logdir DIR] [--seed N] [--cpu]\n",
      "                             [--fp16] [--memory-efficient-fp16]\n",
      "                             [--fp16-no-flatten-grads]\n",
      "                             [--fp16-init-scale FP16_INIT_SCALE]\n",
      "                             [--fp16-scale-window FP16_SCALE_WINDOW]\n",
      "                             [--fp16-scale-tolerance FP16_SCALE_TOLERANCE]\n",
      "                             [--min-loss-scale D]\n",
      "                             [--threshold-loss-scale THRESHOLD_LOSS_SCALE]\n",
      "                             [--user-dir USER_DIR]\n",
      "                             [--empty-cache-freq EMPTY_CACHE_FREQ]\n",
      "                             [--all-gather-list-size ALL_GATHER_LIST_SIZE]\n",
      "                             [--model-parallel-size N]\n",
      "                             [--checkpoint-suffix CHECKPOINT_SUFFIX]\n",
      "                             [--criterion {cross_entropy,adaptive_loss,legacy_masked_lm_loss,nat_loss,label_smoothed_cross_entropy,composite_loss,binary_cross_entropy,sentence_prediction,label_smoothed_cross_entropy_with_alignment,masked_lm,sentence_ranking,vocab_parallel_cross_entropy}]\n",
      "                             [--tokenizer {nltk,space,moses}]\n",
      "                             [--bpe {sentencepiece,fastbpe,gpt2,subword_nmt,hf_byte_bpe,bert,byte_bpe,characters,bytes}]\n",
      "                             [--optimizer {nag,adafactor,sgd,adamax,adagrad,adam,lamb,adadelta}]\n",
      "                             [--lr-scheduler {fixed,reduce_lr_on_plateau,polynomial_decay,inverse_sqrt,tri_stage,cosine,triangular}]\n",
      "                             [--task TASK] [--num-workers N]\n",
      "                             [--skip-invalid-size-inputs-valid-test]\n",
      "                             [--max-tokens N] [--max-sentences N]\n",
      "                             [--required-batch-size-multiple N]\n",
      "                             [--dataset-impl FORMAT] [--gen-subset SPLIT]\n",
      "                             [--num-shards N] [--shard-id ID] [--path FILE]\n",
      "                             [--remove-bpe [REMOVE_BPE]] [--quiet]\n",
      "                             [--model-overrides DICT] [--results-path RESDIR]\n",
      "                             [--beam N] [--nbest N] [--max-len-a N]\n",
      "                             [--max-len-b N] [--min-len N]\n",
      "                             [--match-source-len] [--no-early-stop]\n",
      "                             [--unnormalized] [--no-beamable-mm]\n",
      "                             [--lenpen LENPEN] [--unkpen UNKPEN]\n",
      "                             [--replace-unk [REPLACE_UNK]] [--sacrebleu]\n",
      "                             [--score-reference] [--prefix-size PS]\n",
      "                             [--no-repeat-ngram-size N] [--sampling]\n",
      "                             [--sampling-topk PS] [--sampling-topp PS]\n",
      "                             [--temperature N] [--diverse-beam-groups N]\n",
      "                             [--diverse-beam-strength N] [--diversity-rate N]\n",
      "                             [--print-alignment] [--print-step]\n",
      "                             [--iter-decode-eos-penalty N]\n",
      "                             [--iter-decode-max-iter N]\n",
      "                             [--iter-decode-force-max-iter]\n",
      "                             [--iter-decode-with-beam N]\n",
      "                             [--iter-decode-with-external-reranker]\n",
      "                             [--retain-iter-history]\n",
      "                             [--decoding-format {unigram,ensemble,vote,dp,bs}]\n",
      "                             [--momentum M] [--weight-decay WD]\n",
      "                             [--force-anneal N] [--lr-shrink LS]\n",
      "                             [--warmup-updates N] [-s SRC] [-t TARGET]\n",
      "                             [--load-alignments] [--left-pad-source BOOL]\n",
      "                             [--left-pad-target BOOL]\n",
      "                             [--max-source-positions N]\n",
      "                             [--max-target-positions N]\n",
      "                             [--upsample-primary UPSAMPLE_PRIMARY]\n",
      "                             [--truncate-source] [--eval-bleu]\n",
      "                             [--eval-bleu-detok EVAL_BLEU_DETOK]\n",
      "                             [--eval-bleu-detok-args JSON]\n",
      "                             [--eval-tokenized-bleu]\n",
      "                             [--eval-bleu-remove-bpe [EVAL_BLEU_REMOVE_BPE]]\n",
      "                             [--eval-bleu-args JSON]\n",
      "                             [--eval-bleu-print-samples]\n",
      "                             data\n",
      "ipykernel_launcher.py: error: unrecognized arguments: --raw-text\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def memoize_simplifier(simplifier):\n",
    "    memo = {}\n",
    "\n",
    "    @wraps(simplifier)\n",
    "\n",
    "\n",
    "    return wrapped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessors=[LengthRatioPreprocessor(target_ratio=0.25), LevenshteinPreprocessor(bucket_size=0.05, noise_std=0, target_ratio=0.25), WordRankRatioPreprocessor(target_ratio=0.25), SentencePiecePreprocessor(input_filepaths=None, vocab_size=10000)]\n",
      "simplifier_type=\"fairseq_simplifier\"  \n",
      "exp_dir=\"/Users/alex/personal_projects/access/resources/models/best_model\"  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No traceback available to show.\n",
      "usage: ipykernel_launcher.py [-h] [--no-progress-bar] [--log-interval N]\n",
      "                             [--log-format {json,none,simple,tqdm}]\n",
      "                             [--tensorboard-logdir DIR] [--seed N] [--cpu]\n",
      "                             [--fp16] [--memory-efficient-fp16]\n",
      "                             [--fp16-no-flatten-grads]\n",
      "                             [--fp16-init-scale FP16_INIT_SCALE]\n",
      "                             [--fp16-scale-window FP16_SCALE_WINDOW]\n",
      "                             [--fp16-scale-tolerance FP16_SCALE_TOLERANCE]\n",
      "                             [--min-loss-scale D]\n",
      "                             [--threshold-loss-scale THRESHOLD_LOSS_SCALE]\n",
      "                             [--user-dir USER_DIR]\n",
      "                             [--empty-cache-freq EMPTY_CACHE_FREQ]\n",
      "                             [--all-gather-list-size ALL_GATHER_LIST_SIZE]\n",
      "                             [--model-parallel-size N]\n",
      "                             [--checkpoint-suffix CHECKPOINT_SUFFIX]\n",
      "                             [--criterion {cross_entropy,adaptive_loss,legacy_masked_lm_loss,nat_loss,label_smoothed_cross_entropy,composite_loss,binary_cross_entropy,sentence_prediction,label_smoothed_cross_entropy_with_alignment,masked_lm,sentence_ranking,vocab_parallel_cross_entropy}]\n",
      "                             [--tokenizer {nltk,space,moses}]\n",
      "                             [--bpe {sentencepiece,fastbpe,gpt2,subword_nmt,hf_byte_bpe,bert,byte_bpe,characters,bytes}]\n",
      "                             [--optimizer {nag,adafactor,sgd,adamax,adagrad,adam,lamb,adadelta}]\n",
      "                             [--lr-scheduler {fixed,reduce_lr_on_plateau,polynomial_decay,inverse_sqrt,tri_stage,cosine,triangular}]\n",
      "                             [--task TASK] [--num-workers N]\n",
      "                             [--skip-invalid-size-inputs-valid-test]\n",
      "                             [--max-tokens N] [--max-sentences N]\n",
      "                             [--required-batch-size-multiple N]\n",
      "                             [--dataset-impl FORMAT] [--gen-subset SPLIT]\n",
      "                             [--num-shards N] [--shard-id ID] [--path FILE]\n",
      "                             [--remove-bpe [REMOVE_BPE]] [--quiet]\n",
      "                             [--model-overrides DICT] [--results-path RESDIR]\n",
      "                             [--beam N] [--nbest N] [--max-len-a N]\n",
      "                             [--max-len-b N] [--min-len N]\n",
      "                             [--match-source-len] [--no-early-stop]\n",
      "                             [--unnormalized] [--no-beamable-mm]\n",
      "                             [--lenpen LENPEN] [--unkpen UNKPEN]\n",
      "                             [--replace-unk [REPLACE_UNK]] [--sacrebleu]\n",
      "                             [--score-reference] [--prefix-size PS]\n",
      "                             [--no-repeat-ngram-size N] [--sampling]\n",
      "                             [--sampling-topk PS] [--sampling-topp PS]\n",
      "                             [--temperature N] [--diverse-beam-groups N]\n",
      "                             [--diverse-beam-strength N] [--diversity-rate N]\n",
      "                             [--print-alignment] [--print-step]\n",
      "                             [--iter-decode-eos-penalty N]\n",
      "                             [--iter-decode-max-iter N]\n",
      "                             [--iter-decode-force-max-iter]\n",
      "                             [--iter-decode-with-beam N]\n",
      "                             [--iter-decode-with-external-reranker]\n",
      "                             [--retain-iter-history]\n",
      "                             [--decoding-format {unigram,ensemble,vote,dp,bs}]\n",
      "                             [--momentum M] [--weight-decay WD]\n",
      "                             [--force-anneal N] [--lr-shrink LS]\n",
      "                             [--warmup-updates N] [-s SRC] [-t TARGET]\n",
      "                             [--load-alignments] [--left-pad-source BOOL]\n",
      "                             [--left-pad-target BOOL]\n",
      "                             [--max-source-positions N]\n",
      "                             [--max-target-positions N]\n",
      "                             [--upsample-primary UPSAMPLE_PRIMARY]\n",
      "                             [--truncate-source] [--eval-bleu]\n",
      "                             [--eval-bleu-detok EVAL_BLEU_DETOK]\n",
      "                             [--eval-bleu-detok-args JSON]\n",
      "                             [--eval-tokenized-bleu]\n",
      "                             [--eval-bleu-remove-bpe [EVAL_BLEU_REMOVE_BPE]]\n",
      "                             [--eval-bleu-args JSON]\n",
      "                             [--eval-bleu-print-samples]\n",
      "                             data\n",
      "ipykernel_launcher.py: error: unrecognized arguments: --raw-text\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alex/miniconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3275: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "%tb\n",
    "simplifier(source_filepath, pred_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "simplifier = get_fairseq_simplifier(best_model_dir)\n",
    "simplifier = get_preprocessed_simplifier(simplifier, preprocessors=preprocessors)\n",
    "# \n",
    "pred_filepath = get_temp_filepath()\n",
    "print(pred_filepath)\n",
    "# with mute():\n",
    "#     simplifier(source_filepath, pred_filepath)\n",
    "# for line in yield_lines(pred_filepath):\n",
    "#     print(line)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
